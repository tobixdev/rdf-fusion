use crate::EncodingName;
use crate::encoding::TermEncoding;
use crate::object_id::{
    ObjectIdArray, ObjectIdMapping, ObjectIdMappingError, ObjectIdMappingRef,
    ObjectIdScalar, ObjectIdSize,
};
use crate::plain_term::{PlainTermArray, PlainTermScalar};
use datafusion::arrow::array::ArrayRef;
use datafusion::arrow::datatypes::DataType;
use datafusion::common::ScalarValue;
use rdf_fusion_model::DFResult;
use std::clone::Clone;
use std::hash::{Hash, Hasher};
use std::sync::Arc;

/// A cheaply cloneable reference to a [`ObjectIdEncoding`].
pub type ObjectIdEncodingRef = Arc<ObjectIdEncoding>;

/// The [`ObjectIdEncoding`] represents each distinct term in the database with a single fixed-size
/// id. We call such an id *object id*. Here is an example of the encoding:
///
/// ```text
/// ?variable
///
///  ┌─────┐
///  │   1 │ ────►  <#MyEntity>
///  ├─────┤
///  │   2 │ ────►  120^^xsd:integer
///  ├─────┤
///  │ ... │
///  └─────┘
/// ```
///
/// # Object ID Mapping
///
/// The mapping implementation depends on the storage layer that is being used. For example, an
/// in-memory RDF store will use a different implementation as an on-disk RDF store. The
/// [`ObjectIdMapping`](crate::object_id::ObjectIdMapping) trait defines the contract.
///
/// # Strengths and Weaknesses
///
/// The object id encoding is very well suited for evaluating joins, as instead of joining
/// variable-length RDF terms, we can directly join the object ids. While we do not have recent
/// numbers for the performance gains, the [original pull request](https://github.com/tobixdev/rdf-fusion/pull/27)
/// quadrupled the performance of some queries (with relatively small datasets!).
///
/// However, this also introduces the necessity of decoding the object ids back to RDF terms. For
/// example, by converting it to the [`PlainTermEncoding`](crate::plain_term::PlainTermEncoding).
/// For queries that spend little time on join operations, the cost of decoding the object ids can
/// outweigh the benefits of using the object id encoding.
///
/// Furthermore, the encoding introduces the necessity of maintaining the
/// [`ObjectIdMapping`](crate::object_id::ObjectIdMapping), which can be non-trivial.
///
/// # Equality
///
/// The equality and hashing functions check for pointer equality of the underlying mapping.
///
/// # Current Limitation
///
/// Currently, this id is fixed to being a 32-bit integer. However, we have an
/// [issue](https://github.com/tobixdev/rdf-fusion/issues/50) that tracks the progress on limiting
/// this limitation.
#[derive(Debug, Clone)]
pub struct ObjectIdEncoding {
    /// The number of bytes in a single object id.
    object_id_size: ObjectIdSize,
    /// The mapping that is used to encode and decode object ids.
    mapping: Arc<dyn ObjectIdMapping>,
}

impl ObjectIdEncoding {
    /// Creates a new [ObjectIdEncoding].
    pub fn new(mapping: Arc<dyn ObjectIdMapping>) -> Self {
        Self {
            object_id_size: mapping.object_id_size(),
            mapping,
        }
    }

    /// Returns the size of the object id.
    pub fn object_id_size(&self) -> ObjectIdSize {
        self.object_id_size
    }

    /// Returns the mapping that is used to encode and decode object ids.
    pub fn mapping(&self) -> &ObjectIdMappingRef {
        &self.mapping
    }

    /// Encodes a [`PlainTermScalar`] into an [`ObjectIdScalar`].
    ///
    /// See also [`ObjectIdMapping::encode_scalar`].
    pub fn encode_scalar(
        self: &Arc<Self>,
        scalar: &PlainTermScalar,
    ) -> Result<ObjectIdScalar, ObjectIdMappingError> {
        let Some(object_id) = self.mapping.encode_scalar(scalar)? else {
            return Ok(ObjectIdScalar::null(Arc::clone(self)));
        };

        let bytes = object_id
            .as_bytes()
            .try_into()
            .expect("Currently only 4-byte object ids are supported.");
        Ok(ObjectIdScalar::try_new(
            Arc::clone(self),
            ScalarValue::UInt32(Some(u32::from_be_bytes(bytes))),
        )
        .unwrap())
    }

    /// Encodes a [`PlainTermArray`] into an [`ObjectIdArray`].
    ///
    /// See also [`ObjectIdMapping::encode_array`].
    pub fn encode_array(
        self: &Arc<Self>,
        array: &PlainTermArray,
    ) -> Result<ObjectIdArray, ObjectIdMappingError> {
        self.mapping.encode_array(array).map(|oids| {
            ObjectIdArray::try_new(Arc::clone(self), Arc::new(oids) as ArrayRef).unwrap()
        })
    }
}

impl TermEncoding for ObjectIdEncoding {
    type Array = ObjectIdArray;
    type Scalar = ObjectIdScalar;

    fn name(&self) -> EncodingName {
        EncodingName::PlainTerm
    }

    fn data_type(&self) -> &DataType {
        &DataType::UInt32
    }

    fn try_new_array(self: &Arc<Self>, array: ArrayRef) -> DFResult<Self::Array> {
        ObjectIdArray::try_new(Arc::clone(self), array)
    }

    fn try_new_scalar(self: &Arc<Self>, scalar: ScalarValue) -> DFResult<Self::Scalar> {
        ObjectIdScalar::try_new(Arc::clone(self), scalar)
    }
}

impl PartialEq for ObjectIdEncoding {
    fn eq(&self, other: &Self) -> bool {
        self.object_id_size == other.object_id_size
            && Arc::ptr_eq(&self.mapping, &other.mapping)
    }
}

impl Eq for ObjectIdEncoding {}

impl Hash for ObjectIdEncoding {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.object_id_size.hash(state);
    }
}
